{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGWHWvKdbp9W"
      },
      "outputs": [],
      "source": [
        "#overfitting :this occurs when a model learns the training data too well,captures noise and outliers instead of capturing the pattern.this leads to the poor performance on the test data or unseen data .and this model gives high accuracy score on the train data and less accuracy score on the test data\n",
        "#mitigation: use simpler models with fewer parameters,by appkying regularization techniques(L1 or L2),using cross-validation to ensure the model generalizes well or gather more training data if possible\n",
        "#underfitting: this occurs when the model is to siumple and captures the trend .\n",
        "#mitigation : increase model complexity by adding more features ,redce regularization if it is too strong or ensure that the model is trained for enough epochs to learn from the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "how can we reduce overfitting?\n"
      ],
      "metadata": {
        "id": "vj4nkERcf_fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Techniques to Reduce Overfitting:\n",
        "#Cross-Validation,Regularization (L1/L2),Simpler Models.\n",
        "#1-cross-validation : by splitting your data into multiple folds and training our model on different combinations we get a more robust estimate of its performance and reduce the chance of overfitting\n",
        "#2-Regularization(L1/L2) : Regularization adds a penalty to the model's complexity,discouraging it from learning overly intricate patteerns that might lead to overfitting.\n",
        "#3-simpler Models : sometimes a simpler models like LR moght be sufficient and less prone to overfitting ,especially if we have small data set\n",
        "#4- by gathering the train data if possible then there is chance to reduce the overfitting of the model"
      ],
      "metadata": {
        "id": "0X_FrNEEf2CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain underfitting. List scenarios where underfitting can occur in ML."
      ],
      "metadata": {
        "id": "evFbqtodiZnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#underfitting : where a model is too simpole to capture thee underlying trend\n",
        "#scenarios where underfitting can occur :\n",
        "# 1-if the model is used for training is too simple for complexity data ,it may nit be able to capture the underlying patterns.\n",
        "#2- if the model is not trained on enough data\n",
        "#3-if the model does not have enough featires to capture the patterns in the data"
      ],
      "metadata": {
        "id": "dGAfH6XRiTQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
        "variance, and how do they affect model performance?"
      ],
      "metadata": {
        "id": "tt4mkfikjxl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bias: Error due to overly simplistic assumptions in the learning algorithm, leading to underfitting\n",
        "#Variance: Error due to excessive complexity in the model, leading to overfitting.\n",
        "# Understanding this tradeoff is crucial because it helps in selecting the right model complexity and tuning hyperparameters to achieve optimal performance.\n",
        "#Generally, there is an inverse relationship between bias and variance .\n"
      ],
      "metadata": {
        "id": "jABf6dNIjuPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
        "How can you determine whether your model is overfitting or underfitting?"
      ],
      "metadata": {
        "id": "nNnaSsE9kSRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#detecting overfitting and underfitting :\n",
        "#1-train-test-split data:spliting the data set into training,validation,and testinf data set ,in this train the model on the training dataset amd evalutes the model performance on the validation set during traing and the model is tested on the test data\n",
        "#2-learning curves : plot the models performace on the training and validation datasets ,so that we can know if our model is overfitting or underfitting\n",
        "#3- cross-validation : use k-fold cross-validation to train and evaluate the model on different substes\n"
      ],
      "metadata": {
        "id": "6F5T5bKTkUu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
        "and high variance models, and how do they differ in terms of their performance?"
      ],
      "metadata": {
        "id": "T5IAtO6lmC8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#High Bias: Models with high bias are too simplistic, failing to capture the underlying patterns in the data.\n",
        "#They tend to underfit, resulting in poor performance on both training and test datasets\n",
        " #High Variance: Models with high variance are overly complex, capturing noise in the training data. They\n",
        "#perform well on training data but poorly on unseen data, leading to overfitting"
      ],
      "metadata": {
        "id": "VHIiAdMwmHNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
        "some common regularization techniques and how they work."
      ],
      "metadata": {
        "id": "C3rLorDomskV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularization is a technique used to discourage overly complex models by adding a penalty term to the loss function.\n",
        "#This penalty discourages large coefficients in the model, effectively simplifying it.\n",
        "#Regularization helps prevent overfitting by ensuring that the model generalizes better to new data.\n",
        "#common regularization techniques are L1 Regularization(Lasso) and L2 Regularization (Ridge)\n",
        "#Lasso : adds a penalty proportional to the absolute values of the model's weights\n",
        "#ridge : adds a penalty proportional to the square of the model's weights."
      ],
      "metadata": {
        "id": "x35VxoGgmzX_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}